import asyncio
import requests
from pystyle import *
from bs4 import BeautifulSoup

import aiohttp

KRAKEN = Write.Input("KrakenFiles Link > ", Colors.blue, interval=0)
SCRAPE = requests.get(KRAKEN)
print(" ")
AMOUNT = Write.Input("Download Amount > ", Colors.blue, interval=0)

print(" ")
Write.Print("this may take a while, please wait", Colors.purple, interval=0)
print(' ')

soup = BeautifulSoup(SCRAPE.content, "html.parser")

async def fetch(s, url):
    downloads_sent = 0
    get_token = soup.find("input", attrs={"id":"dl-token"})["value"]
    data = {"token":f"{get_token}"}
    link = soup.find("form", attrs={"id":"dl-form"})["action"]
    async with s.post(f"https:{link}", data=data) as r:
        if r.status != 200:
            r.raise_for_status()
        else:
            downloads_sent = downloads_sent + 1
            Write.Print(f"{downloads_sent} Downloads Sent! \n", Colors.green, interval=0)
        return await r.text()


async def fetch_all(s, urls):
    tasks = []
    for url in urls:
        task = asyncio.create_task(fetch(s, url))
        tasks.append(task)
    res = await asyncio.gather(*tasks)
    return res


async def main():
    urls = range(1, int(AMOUNT))
    async with aiohttp.ClientSession() as session:
        htmls = await fetch_all(session, urls)
        print(htmls)


if __name__ == '__main__':
    asyncio.run(main())
